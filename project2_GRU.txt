#!/usr/bin/env python
# coding: utf-8

# In[2]:


get_ipython().system('pip install contractions')
get_ipython().system('pip install nltk')


# In[31]:


import pandas as pd
import numpy as np
import re
import string
import contractions
import nltk
from nltk.corpus import stopwords
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.text import Tokenizer 
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.layers import Input,LSTM,Embedding,Dense,Concatenate,TimeDistributed,Bidirectional,Attention,GRU
from tensorflow.keras.models import Model
from tensorflow.keras.callbacks import EarlyStopping
from Attention import AttentionLayer
nltk.download('stopwords')


# In[ ]:


dataset=pd.read_csv("./Reviews.csv",nrows=100000)
dataset=dataset.drop(["Id","ProductId","UserId","ProfileName","HelpfulnessNumerator","HelpfulnessDenominator","Score","Time"],axis=1)
dataset=dataset.dropna(axis=0)


# In[5]:


dataset.head(10)


# In[ ]:


def preprocess_text(dataset):
    cleaned_text=[]
    for data in dataset:
        cleaned_data = data.lower()
        #remove htmltags
        html_tag = re.compile('<.*?>')
        cleaned_data= re.sub(html_tag,' ',cleaned_data)       
        cleaned_data=cleaned_data.strip()      
        #remove paranthsis data
        cleaned_data = re.sub(r'\([^)]*\)',' ', cleaned_data)
        cleaned_data = re.sub('"','',cleaned_data)
        #expand contractions
        cleaned_data=contractions.fix(cleaned_data)
        #remove punctuations
        cleaned_data = re.sub(r"'s\b"," ",cleaned_data)
        cleaned_data = cleaned_data.translate(str.maketrans(' ', ' ', string.punctuation))
        cleaned_text.append(cleaned_data)
    return cleaned_text


# In[8]:


cleaned_reviews=[]
stop_words = set(stopwords.words('english')) 
cleaned_text_data=preprocess_text(dataset["Text"])
for text in cleaned_text_data:
    filtered_sentence = [w for w in text.split() if not w in stop_words] 
    long_words=[]
    for i in filtered_sentence:
        if len(i)>=3:   
            long_words.append(i)   
    cleaned_reviews.append(" ".join(long_words))
cleaned_reviews[:10]


# In[9]:


cleaned_Summary_data=preprocess_text(dataset["Summary"])
cleaned_Summary_data[:10]


# In[ ]:


summary_tag=[]
for summary in cleaned_Summary_data:
    summary="<START> "+summary+" <END>"
    summary_tag.append(summary)
dataset["Text"]=cleaned_reviews
dataset["Summary"]=summary_tag


# In[13]:


text_maximum_length=100
summary_maximum_length=10
dataset.info()


# In[ ]:


train_text,test_text,train_summary,test_summary=train_test_split(cleaned_reviews,summary_tag,test_size=0.3)


# In[ ]:


review_tokenizer = Tokenizer()
review_tokenizer.fit_on_texts(list(train_text))


# In[18]:


train_text[1]


# In[19]:


test_text[1]


# In[20]:


print(train_summary[1])


# In[21]:


test_summary[1]


# In[ ]:


train_text=review_tokenizer.texts_to_sequences(train_text) 
train_text=pad_sequences(train_text,maxlen=text_maximum_length,padding='post') 
test_text=review_tokenizer.texts_to_sequences(test_text)
test_text=pad_sequences(test_text,maxlen=text_maximum_length,padding='post')


# In[24]:


review_vocab_size=len(review_tokenizer.word_index) +1
review_vocab_size


# In[25]:


summary_tokenizer = Tokenizer()
summary_tokenizer.fit_on_texts(list(train_summary))
train_summary=summary_tokenizer.texts_to_sequences(train_summary) 
train_summary=pad_sequences(train_summary,maxlen=summary_maximum_length,padding='post')
test_summary=summary_tokenizer.texts_to_sequences(test_summary)
test_summary=pad_sequences(test_summary,maxlen=summary_maximum_length,padding='post')
summary_vocab_size=len(summary_tokenizer.word_index) +1
latent_dim=500
summary_vocab_size


# In[35]:


review_inputs=Input(shape=(text_maximum_length,)) 
input_embedding=Embedding(review_vocab_size,latent_dim,trainable=True)(review_inputs) 
gru1=GRU(latent_dim,return_sequences=True,return_state=True)
enc_out1,h1=gru1(input_embedding)
gru2=GRU(latent_dim,return_sequences=True,return_state=True)
enc_out2,h2=gru2(enc_out1)
gru3=GRU(latent_dim,return_sequences=True,return_state=True)
enc_out3,h3=gru33(enc_out2)

dec_input=Input(shape=(None,))
dec_embedding_lyr=Embedding(summary_vocab_size, latent_dim,trainable=True)
dec_embedding=dec_embedding_lyr(dec_input)
dec_gru1=GRU(latent_dim,return_sequences=True,return_state=True)
dec_out,_=dec_gru1(dec_embedding,initial_state=[h3]) 
attention_layer=AttentionLayer()
attn_out,_= attention_layer([enc_out3, dec_out])

decoder_concat_input=Concatenate()([dec_out,attn_out])
dense=TimeDistributed(Dense(summary_vocab_size, activation='softmax'))
dec_output=dense(decoder_concat_input) 
model=Model([review_inputs,dec_input],dec_output) 
model.summary()


# In[ ]:


model.compile(optimizer='Adam', loss='sparse_categorical_crossentropy')
es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)


# In[49]:


history=model.fit([train_text,train_summary[:,:-1]], train_summary.reshape(train_summary.shape[0],train_summary.shape[1], 1)[:,1:] ,epochs=50,callbacks=[es],batch_size=128, validation_data=([test_text,test_summary[:,:-1]], test_summary.reshape(test_summary.shape[0],test_summary.shape[1], 1)[:,1:]))


# In[50]:


from matplotlib import pyplot 
pyplot.plot(history.history['loss'], label='train') 
pyplot.plot(history.history['val_loss'], label='test') 
pyplot.legend() 
pyplot.show()


# In[ ]:


model.save("model_GRU.h5")


# In[ ]:


encoder_model = Model(inputs=review_inputs,outputs=[enc_out3, h3])

dec_in_h = Input(shape=(latent_dim,))
dec_in_c = Input(shape=(latent_dim,))
dec_hidden_in = Input(shape=(text_maximum_length,latent_dim))

dec_emb2= dec_embedding_lyr(dec_input)

dec_out2,h= dec_lstm1(dec_emb2, initial_state=[dec_in_h])

attn_out_inf,_ = attention_layer([dec_hidden_in, dec_out2])

decoder_inf_concat = Concatenate()([dec_outp2, attn_out_inf])
decoder_outputs2 = dense(decoder_inf_concat)
decoder_model = Model([dec_input] + [dechidden_input,dec_in_h],[dec_out2] + [h])


# In[ ]:


reverse_target_word=summary_tokenizer.index_word 
reverse_source_word=review_tokenizer.index_word 
target_word=summary_tokenizer.word_index
source_word=review_tokenizer.word_index


# In[ ]:


ground_truth_review=[]
ground_truth_summery=[]
predicted_summary=[]
for n in range(len(test_text)):
    givenReview=''
    for review_token in test_text[n]:
        if(review_token!=target_word['start'] and review_token!=target_word['end']):
            givenReview=givenReview+reverse_source_word[review_token]+' '
    ground_truth_review.append(givenReview)
    givenSummary=''
    for summary_token in test_summary[n]:
        if(summary_token!=target_word['start'] and summary_token!=target_word['end']):
            givenSummary=givenSummary+reverse_target_word[summary_token]+' '
    ground_truth_summery.append(givenSummary)        
    e_out, e_h= encoder_model.predict(test_text[n].reshape(1,100))
    target_seq = np.zeros((1,1))
    target_seq[0][0] = target_word_index['start']
    decoded_sentence = ''
    while True:
        output_tokens, h = decoder_model.predict([target_seq] + [e_out, e_h])
        sampled_token_index = np.argmax(output_tokens[0, -1, :])
        sampled_token = reverse_target_word_index[sampled_token_index]
        if(sampled_token!='end'):
            decoded_sentence += ' '+sampled_token
        if (sampled_token == 'end' or len(decoded_sentence.split()) >= (summary_maximum_length-1)):
            break
        target_seq = np.zeros((1,1))
        target_seq[0, 0] = sampled_token_index
        e_h = h
    predicted_summary.append(decoded_sentence)


# In[59]:


for i in range(15):
    print("\n")
    print("Review: {}".format(ground_truth_review[i]))
    print("Actual Summary: {}".format(ground_truth_summery[i]))
    print("Predicted Summary:{}".format(predicted_summary[i]))
    


# In[60]:


from nltk.translate.bleu_score import sentence_bleu
from statistics import mean 
score=[]
for i in range(100):
    reference=list(ground_truth_summery[i].split())
    candidate=list(predicted_summary[i].split())
    score.append(sentence_bleu(reference,candidate,weights=(1,0,0,0)))

mean(score)


# In[ ]:


from pythonrouge.pythonrouge import Pythonrouge
for i in range(100):
    reference=ground_truth_summery[i]
    candidate=predicted_summary[i]
    rouge = Pythonrouge(summary_file_exist=False,
                    summary=candidate, reference=reference,
                    n_gram=1, ROUGE_SU4=True, ROUGE_L=False,
                    recall_only=True, stemming=True, stopwords=True,
                    word_level=True, length_limit=True, length=50,
                    use_cf=False, cf=95, scoring_formula='average',
                    resampling=True, samples=1000, favor=True, p=0.5)
    score = rouge.calc_score()
    print(score)


# In[ ]:




